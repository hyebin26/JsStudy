검색 시스템과 Googlebot이 작동하는 방식에 대한 일반적인 개요가 있으므로 크롤링 및 indexing 생성에 영향을 미치는 몇 가지 주요 부분에 대해 자세히 알아보겠습니다.

- HTTP status code 및 기본사항
- metadata와 웹 크롤러가 웹 컨텐츠를 찾기 위한 항목
- 사이트에 새 콘텐츠가 있을 때 검색 크롤러가 알 수 있도록 Google과 통신하는 방법
- 메타 로봇 태그와 표준 링크를 활용하여 원하는 인덱싱 상태를 검색 엔진에 표시하는 법

### HTTP 상태 코드에 대해서

- 200은 요청이 성공했음을 나타냅니다.
- 301/308은 요청된 리소스가 대상 URL로 확실히 이동되었음을 나타냅니다. 영구 리다이렉션으로, 가장 널리 사용되는 리다이렉션 유형입니다.
- 302는 일시적인 리다이렉션
- 404는 요청된 리소스를 찾을 수 없음을 나타냅니다.
- 410은 클라이언트 오류 응답 코드로, 대상 리소스에 대한 액세스가 원본 서버에서 더 이상 사용할 수 없으며, 이 상태가 영구적일 가능성이 있음을 나타냅니다. 예를 들면, 블로그 게시물이 제거되었을 때, 전자상거래에서 영구적으로 제거된 제품 등이 있습니다.
- 503은 서버 오류 응답코드로, 서버가 요청을 처리할 준비가 되지 않았음을 의미합니다.

### robots.txt 파일은?

robots.txt 파일은 페이지 혹은 파일들을 크롤러가 사이트로부터 요청할 수 있는지 혹은 요청할 수 없는지 검색엔진 크롤러에게 말해준다. 이 파일은 대부분의 봇이 특정 도메인에서 무엇이든지 요청하기 전에 사용하는 웹 표준파일입니다.

CMS 또는 관리자, 전자 상거래의 사용자 계정 또는 일부 API경로와 같은 웹사이트의 특정 영역이 크롤링되어 indexing이 생성되지 않도록 보호할 수 있습니다.

이러한 파일들은 루트폴더에 위치해야 합니다. 그렇지 않으면 루트 ‘/robots.txt`경로를 대상 URL로 리다이렉션할 수 있으며 대부분 봇이 이를 따릅니다.

Next.js에서 robots.txt를 추가하는 방법은 루트 디렉토리에 robots.txt라는 파일을 추가하면 쉽게 파일을 추가할 수 있습니다.

### XML Sitemaps

Sitemap은 Google과 소통하는 가장 쉬운 방법입니다. Google이 새 콘텐츠를 쉽게 감지하고 웹사이트를 보다 효율적으로 크롤링할 수 있도록 웹사이트에 속한 URL과 업데이트 시기를 나타냅니다.

사이트맵은 사이트의 페이지, 동영상 및 기타 파일과 이들 간의 관계에 대한 정보를 제공하는 파일입니다. Google과 같은 검색 엔진은 이 파일을 읽고 사이트를 보다 지능적으로 크롤링합니다. 사이트맵이 필요한 경우는

- 사이트가 클 경우 Google웹 크롤러가 새 페이지 또는 최근에 업데이트된 페이지의 일부 크롤링을 간과할 가능성이 있습니다.
- 사이트가 신규 사이트이며 이에 대한 외부 링크가 거의 없습니다. Googlebot및 기타 웹 크롤러는 한 페이지에서 다른 페이지로의 링크를 따라 웹을 탐색합니다.. 결과적으로 다른 사이트에 링크되지 않은 경우 Google에서 귀하의 페이지를 찾지 못할 수 있습니다.
- 사이트에 리치 미디어 컨텐츠(동영상, 이미지)가 많거나 Google 뉴스에 표시됩니다. 제공되는 경우 Google은 해당되는 경우 검색을 위해 사이트맵의 추가 정보를 고려할 수 있습니다.

사이트맵은 필수는 아니지만 봇에 대한 크롤링과 인덱싱을 용이하게 할 수 있으므로 컨텐츠가 더 빨리 선택되고 그에 따라 순위가 지정됩니다.

사이트맵을 사용하고 웹사이트 전체에 새 콘텐츠가 채워질 때 사이트맵을 동적으로 만드는 것을 강하게 추천합니다. 정적인 사이트맵은 유요하나, 지속적인 검색 목적으로 제공되지 않으므로 Google에 덜 유용할 수 있습니다.

### 검색 엔진을 위한 특별한 Meta Tags

meta robot tags는 검색 엔진이 항상 준수하는 지시문입니다. 이러한 로봇 태그를 추가하면 웹사이트의 indexing을 더 쉽게 만들 수 있습니다.

meta robot tags 또는 robots.txt파일들은 지시문이고, 항상 감지된다. 반면에 Canonical tags는 구글이 감지할지 혹은 감지하지 않을 지 결정할 수 있는 권장사항입니다.

[전체 지시문 목록을](https://developers.google.com/search/docs/advanced/robots/robots_meta_tag#directives) 참고

### Canonical 태그란 무엇입니까?

Canonical(표준) URL은 검색 엔진이 페이지에 복제된 세트로부터 가장 대표적으로 생각하는 페이지의 URL입니다.

표준 URL을 검색 엔진에 직접 전달할 수 있지만 사용자가 알리지 않고 여러 URL을 그룹화하도록 결정할 수도 있습니다. 이것은 여러 결로에서 URL을 찾을 수 있는 경우 자동으로 발생할 수 있습니다.

Google에서 컨텐츠가 동일한 URL을 여러 개 찾은 경우 중복된 것으로 간주되서 검색결과에서 해당 URL을 강등하기로 결정할 수도 있습니다. 이는 도메인간에서도 발생하며, 두 개의 서로 다른 웹사이트를 운영하고 각각에 동일한 컨텐츠를 게시하면 검색 엔진에서 순위를 매길 웹사이트 중 하나를 선택하거나 둘 모두를 직접 강등하도록 결정할 수 있습니다.

여기에서 표준 태그가 사용되며, 어떤 URL이 원본 정보 출처인지 Google에 알립니다. 동일하거나 다른 도메인에서 중복된 페이지가 많으며 순위가 나빠지거나 불이익을 받을 수도 있습니다.

예를 들어, 전자상거래 상점에서 [example.com/products/phone](http://example.com/products/phone) 및 [example.com/phone을](http://example.com/phone을) 통해 제품에 액세스할 수 있도록 가정하면, 둘 다 유효하고, 작동하는 URL이지만 우리가 소유한 중복 컨텐츠의 감지를 방지하기 위해 표준(Canonical)을 사용합니다. 

```jsx
<link rel="canonical" href="https://example.com/products/phone" />
```

canonical tag는 다양한 URL을 생성할 수 있을 뿐만 아니라 사용자 또는 마케팅 도구로 생성할 수 있기 때문에 SEO성능의 기본입니다.
